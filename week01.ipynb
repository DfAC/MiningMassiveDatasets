{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint ON #pretty printing\n",
    "import pdb #debugger\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mining massive datasets week01\n",
    "-------------------\n",
    "\n",
    "#Map reduce\n",
    "\n",
    "**Map Reduce** is the programming approach to the big data, when we have to distributed the dataset across multiple nodes. This approach is required as with multiple servers (nodes) chance of failure increase rapidly -- we need a way of eliminating this event affecting our calculations. **Map Reduce** simplify programming model, move computation close to the data and maintains redundancy, preventing system failure.\n",
    "The whole approach consist of following steps:\n",
    "\n",
    "* take input values\n",
    "* group input by the key (map) (equivalent of sorting values)\n",
    "* reduce those groups (by for ex by summing them)\n",
    "* output a single value for the key\n",
    "\n",
    "\n",
    "#Page Ranking\n",
    "\n",
    "To solve for simple equations we could use [Gaussian elimination](https://en.wikipedia.org/wiki/Gaussian_elimination), but for any real life problem we need to use matrix approach, in which flow equation can be written as $r = Mr$. This is similar to eigenvector problem, where r is first egienvector (largest possible value == 1) of stochastic web matrix M (web page x links, j x i). This means we can use **power iteration** to compute r\n",
    "\n",
    "\n",
    "\n",
    "<a name=\"Power_Iteration\">\n",
    "##Power iteration\n",
    "</a>\n",
    "\n",
    "* initialise with $r_0= [\\frac{1}{N} \\cdots \\frac{1}{N}]$\n",
    "* iterate $r^{t+1} = Mr^t$ until $| r^{t+1} - r^t | < \\varepsilon$ so using L1 norm\n",
    "    * note that in Matrix M - page on col i point to page on row j.\n",
    "\n",
    "Lets see this coded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#returns converge r, require web matrix M \n",
    "def PowerIteration (M,allowedIterations=300):\n",
    "    \n",
    "  iterationIdx = 0\n",
    "\n",
    "  #init r as (n,1) matrix\n",
    "  numPages= M.shape[0]\n",
    "  r = np.asmatrix(np.ones(numPages)/float(numPages)).T #[1/N...]\n",
    "\n",
    "  #iterate until converge\n",
    "  iterationDiff = 1\n",
    "  while iterationDiff>0:  \n",
    "    if iterationIdx>=allowedIterations:\n",
    "      print \"Iteration limit of %i reached, exiting...\" %iterationIdx\n",
    "      break\n",
    "    r_old = r\n",
    "    r = M *r\n",
    "    iterationDiff = max(abs((r-r_old)))\n",
    "    iterationIdx += 1\n",
    "  return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit of 300 reached, exiting...\n",
      "[[ 0.4]\n",
      " [ 0.4]\n",
      " [ 0.2]]\n"
     ]
    }
   ],
   "source": [
    "#define M for pages (y,a,m)\n",
    "M = np.matrix([[0.5, 0.5, 0],\n",
    "               [.5, 0, 1],\n",
    "               [0, .5, 0]])\n",
    "\n",
    "NormalRanking = PowerIteration(M)\n",
    "print(NormalRanking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = Random_walk>\n",
    "##Random walk\n",
    "</a>\n",
    "If we consider our algorithm from surfer perspective, we have to include probability in previous equation, so for each epoch t we have $p_{t+1}=Mp_t$. Hence our flow equation can be interpreted as stationary distribution of random walk (that is when $p_{t+1}=p_t$) and from Markov processes we know that, if we fulfil all conditions, stationary distribution is unique.\n",
    "\n",
    "Given that, does our algorithm always coverage to corect values? Not always:\n",
    "\n",
    "* spider trap problem - page links to itself absorbing all score importance\n",
    "* dead end problem - problems with web pages without outgoing links, our score is leaked\n",
    "\n",
    "###Spider trap problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit of 300 reached, exiting...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[  9.52055792e-29],\n",
       "        [  5.88402839e-29],\n",
       "        [  1.00000000e+00]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define M with spider trap at page m\n",
    "M = np.matrix([[0.5, 0.5, 0],\n",
    "               [.5, 0, 0],\n",
    "               [0, .5, 1]])\n",
    "\n",
    "SpiderTrapRanking = PowerIteration(M)\n",
    "#SpiderTrapRanking - NormalRanking \n",
    "SpiderTrapRanking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spider trap suck up all our markings, we end up  (0,0,1) problem. Solution to this problem is to add **teleport** and define probability $1-\\beta$ (commoonly 0.1-0.2) when walker will not follow random link but teleport to random page instead.\n",
    "  \n",
    " ###Dead end problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit of 300 reached, exiting...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[  9.52055792e-29],\n",
       "        [  5.88402839e-29],\n",
       "        [  3.63652953e-29]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define M with dead end at page m\n",
    "M = np.matrix([[0.5, 0.5, 0],\n",
    "               [.5, 0, 0],\n",
    "               [0, .5, 0]])\n",
    "\n",
    "DeadEndRanking = PowerIteration(M)\n",
    "DeadEndRanking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our ranking has leaked out and PageRanking does not longer sum to 1. Solution to this problem is to add **teleport** with probability 1 once we read dead-end. Combining both problems we can rewrite our orginal equation as $r_j = \\sum_{i\\rightarrow j} \\beta \\frac{r_i}{d_i}+(1-\\beta) \\frac{1}{n}$ .\n",
    "\n",
    "##Explanation of teleports\n",
    "\n",
    "Folloowing follows markow chains, to do so our matrix M has to be stochastic (all columns sum to 1, teleport allows this for dead ends), irreducible (we cant get stuck in a give state) and aperiodic (there is no repeatability in our data). This is possible by indroducing random jumps, this is  In case of matrix this means introducing new matrix $A = \\beta M +(1-\\beta) \\frac{1}{n}ee^T$, which we then use for flow equation, hence final equation is $r_{t+1}= A r_t$.\n",
    "Code below demonstrate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#returns converge r, require web matrix M, stayProbability, MaxAllowedIterations last value is optional\n",
    "#stayProbability, \\beta - how often will random walker just follow the link\n",
    "#otherwords:: JumpProbability = 1-stayProbability\n",
    "def PowerIteration (M,stayProbability,allowedIterations=300):\n",
    "  covergeAccuracy = 10**-15\n",
    "  iterationDiff = 1\n",
    "  iterationIdx = 1\n",
    "\n",
    "  #pdb.set_trace()\n",
    "  #init r as (n,1) matrix normalised to 1\n",
    "  numPages= M.shape[0]\n",
    "  r = np.asmatrix(np.ones(numPages)/float(numPages)).T #[1/N...]\n",
    "\n",
    "  #calculate matrix A\n",
    "  A = stayProbability*M + np.ones((numPages,numPages))*(1-stayProbability)/float(numPages)\n",
    "\n",
    "  #iterate until converge (within given acc) or no of iterations exceed allowed limit\n",
    "  while (iterationDiff>covergeAccuracy):\n",
    "    if iterationIdx>allowedIterations:\n",
    "      print \"Iteration limit of %i reached, exiting...\" %iterationIdx\n",
    "      break\n",
    "    r_old = r\n",
    "    r = A *r\n",
    "    iterationDiff = max(abs((r-r_old)))\n",
    "    iterationIdx +=1\n",
    "    \n",
    "  return (A,r*numPages) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##What if it does not fit into memory of our machine?\n",
    "\n",
    "Matrix A is the most memory hungry so lets avoid using it. To do so we focus on using matrix M instead and \"tax\" it for leaked ranking after each iteration. To do so we will use equation $r = \\beta M r + [\\frac{1-\\beta}{n}]_n$. This way we only need smaller matrix M for our calculations, corrected after each iteration by $[\\frac{1-\\beta}{n}]_n$, which distribute some of our weight equally to other pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns converge r, require web matrix M, stayProbability, MaxAllowedIterations last value is optional\n",
    "#stayProbability, \\beta - how often will random walker just follow the link\n",
    "#otherwords:: JumpProbability = 1-stayProbability\n",
    "def PowerIteration (M,stayProbability,allowedIterations=300):\n",
    "  covergeAccuracy = 10**-15\n",
    "  iterationDiff = 1\n",
    "  iterationIdx = 1\n",
    "\n",
    "\n",
    "  #init r as (n,1) matrix normalised to 1\n",
    "  numPages= M.shape[0]\n",
    "  r = np.asmatrix(np.ones(numPages)/float(numPages)).T #[1/N...]\n",
    "\n",
    "  #correct matrix M\n",
    "  M = stayProbability*M\n",
    "  \n",
    "  #create leak correction to each r\n",
    "  leakageCorrection = np.asmatrix(np.ones((numPages))*(1-stayProbability)/float(numPages)).T\n",
    "  #pdb.set_trace()\n",
    "  #iterate until converge (within given acc) or no of iterations exceed allowed limit\n",
    "  while (iterationDiff>covergeAccuracy):\n",
    "    if iterationIdx>allowedIterations:\n",
    "      print \"Iteration limit of %i reached, exiting...\" %iterationIdx\n",
    "      break\n",
    "    r_old = r\n",
    "    r = M *r\n",
    "    r = r + leakageCorrection\n",
    "    iterationDiff = max(abs((r-r_old)))\n",
    "    iterationIdx +=1\n",
    "    \n",
    "  return (A,r*numPages) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#Week1 tests\n",
    "##Question 1\n",
    "Suppose we compute PageRank with a β of 0.7, and we introduce the additional constraint that the sum of the PageRanks of the three pages must be 3, to handle the problem that otherwise any multiple of a solution will also be a solution. Compute the PageRanks a, b, and c of the three pages A, B, and C, respectively. Then, identify from the list below, the true statement.\n",
    "![Q1](https://d396qusza40orc.cloudfront.net/mmds/images/otc_pagerank2.gif).\n",
    "\n",
    "##Solution\n",
    "To start with lets define final **PowerIteration** function solving for both dead ends and spider traps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then we can define matrix M, remembering that in Matrix M - page on col i point to page on row j. Hence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3  ]\n",
      " [ 0.405]\n",
      " [ 2.295]]\n",
      "a+c = 2.595\n",
      "b+c = 2.700\n",
      "a+b = 0.705\n"
     ]
    }
   ],
   "source": [
    "M = np.matrix([\n",
    "        [0,0,0],\n",
    "        [.5,0,0],\n",
    "        [0.5,1,1]      \n",
    "    ])\n",
    "A,PageRank = PowerIteration(M,0.7)\n",
    "print PageRank\n",
    "\n",
    "print \"a+c = %.3f\" %(PageRank[0]+PageRank[2])\n",
    "print \"b+c = %.3f\" %(PageRank[1]+PageRank[2])\n",
    "print \"a+b = %.3f\" %(PageRank[0]+PageRank[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Question 2\n",
    "Consider three Web pages with the following links:\n",
    "![Q1](https://d396qusza40orc.cloudfront.net/mmds/images/otc_pagerank3.gif).\n",
    "\n",
    "Suppose we compute PageRank with β=0.85. Write the equations for the PageRanks a, b, and c of the three pages A, B, and C, respectively. Then, identify in the list below, one of the equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05   0.05   0.9  ]\n",
      " [ 0.475  0.05   0.05 ]\n",
      " [ 0.475  0.9    0.05 ]]\n",
      "a= 0.053b+ 0.947c\n",
      "b= 0.500a+ 0.053c\n",
      "c= 0.500a+ 0.947b\n"
     ]
    }
   ],
   "source": [
    "M = np.matrix([\n",
    "        [0,0,1],\n",
    "        [.5,0,0],\n",
    "        [0.5,1,0]      \n",
    "    ])\n",
    "A,PageRank = PowerIteration(M,0.85)\n",
    "\n",
    "#show calculated matrix A\n",
    "print A\n",
    "\n",
    "#convert matrix to equation\n",
    "print \"a= %.3fb+ %.3fc\" %(A[0,1]/(1-A[0,0]),A[0,2]/(1-A[0,0]))\n",
    "print \"b= %.3fa+ %.3fc\" %(A[1,0]/(1-A[1,1]),A[1,2]/(1-A[1,1]))\n",
    "print \"c= %.3fa+ %.3fb\" %(A[2,0]/(1-A[2,2]),A[2,1]/(1-A[2,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Question 3\n",
    "Consider three Web pages with the following links:\n",
    "![Q1](https://d396qusza40orc.cloudfront.net/mmds/images/otc_pagerank3.gif).\n",
    "\n",
    "Assuming no \"taxation,\" compute the PageRanks a, b, and c of the three pages A, B, and C, using iteration, starting with the \"0th\" iteration where all three pages have rank a = b = c = 1. Compute as far as the 5th iteration, and also determine what the PageRanks are in the limit. Then, identify the true statement from the list below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last iteration value:\n",
      "[[ 1.18468906]\n",
      " [ 0.65349285]\n",
      " [ 1.16181809]]\n",
      "\n",
      "Difference from previous iteration:\n",
      "[[ 0.        ]\n",
      " [ 0.05546316]\n",
      " [-0.05546316]]\n"
     ]
    }
   ],
   "source": [
    "M = np.matrix([\n",
    "        [0,0,1],\n",
    "        [.5,0,0],\n",
    "        [0.5,1,0]      \n",
    "    ])\n",
    "r =  np.asmatrix(np.ones(3)).T #[1,1,1]\n",
    "numPages= M.shape[0]\n",
    "\n",
    "for idx in range(5): # 5 iterations\n",
    "  r_old = r\n",
    "  r = A *r\n",
    "  iterationDiff = r - r_old\n",
    "  \n",
    "\n",
    "print \"Last iteration value:\"\n",
    "print r\n",
    "print \"\\nDifference from previous iteration:\"\n",
    "print iterationDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Question 4\n",
    "Suppose our input data to a map-reduce operation consists of integer values (the keys are not important). The map function takes an integer i and produces the list of pairs (p,i) such that p is a prime divisor of i. For example, map(12) = [(2,12), (3,12)].\n",
    "The reduce function is addition. That is, reduce(p, $[i_1, i_2, ...,i_k])$ is $(p,i_1+i_2+...+i_k)$.\n",
    "\n",
    "Compute the output, if the input is the set of integers 15, 21, 24, 30, 49. Then, identify, in the list below, one of the pairs in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetAllPrimeFactors(inputInteger):\n",
    "  idx = 2\n",
    "  factors = []\n",
    "  while idx * idx <= inputInteger:\n",
    "    if inputInteger % idx:\n",
    "      idx += 1\n",
    "    else:\n",
    "      inputInteger //= idx\n",
    "      factors.append(idx)\n",
    "\n",
    "  if inputInteger > 1:\n",
    "    factors.append(inputInteger)\n",
    "        \n",
    "  #get only unique\n",
    "  factors = list(set(factors))\n",
    "  return factors\n",
    "\n",
    "######################################\n",
    "#Our mapping fuction returning (key,integer)\n",
    "def mappingFuction(inputInteger):\n",
    "  allPrimeFactors = GetAllPrimeFactors(inputInteger)\n",
    "  mappedKeys = []\n",
    "  for key in allPrimeFactors:\n",
    "    mappedKeys.append((key,inputInteger))\n",
    "  return mappedKeys\n",
    "\n",
    "######################################\n",
    "#Our reduce fuction returning (sum of integers)\n",
    "def reduceFuction(mappedKeys):\n",
    "  reducedKeys = dict()\n",
    "  lastKey = None\n",
    "  for idx in mappedKeys:\n",
    "    #pdb.set_trace()\n",
    "    key = idx[0]\n",
    "    if key == lastKey: #lets add\n",
    "      reducedKeys[key] += idx[1]\n",
    "    else:\n",
    "      reducedKeys[key] = idx[1] #its first key\n",
    "    lastKey = key\n",
    "\n",
    "  return reducedKeys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Those are our mapped values\n",
      "[(2, 4), (2, 12), (2, 24), (2, 30), (3, 12), (3, 15), (3, 21), (3, 24), (3, 30), (5, 15), (5, 30), (7, 21), (7, 49)]\n",
      "\n",
      "Those are our reduced (final) values\n",
      "{2: 70, 3: 102, 5: 45, 7: 70}\n"
     ]
    }
   ],
   "source": [
    "integers = [4,12,15, 21, 24, 30, 49]\n",
    "\n",
    "#First lets map values\n",
    "\n",
    "mappedValues =  map((lambda x: mappingFuction(x)),integers) #list of lists\n",
    "mappedValues = [val for sublist in mappedValues for val in sublist] #flatten it\n",
    "#then sort them\n",
    "mappedValues = sorted(mappedValues, key=lambda key: key[0])\n",
    "print \"Those are our mapped values\"\n",
    "print mappedValues\n",
    "\n",
    "\n",
    "#Then lest reduce it\n",
    "reducedValues = reduceFuction(mappedValues)\n",
    "print \"\\nThose are our reduced (final) values\"\n",
    "print reducedValues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Interesting links to follow\n",
    "\n",
    "* Old way of detecting primes [Sieve_of_Eratosthenes](https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
