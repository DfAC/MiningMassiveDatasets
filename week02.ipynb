{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pdb #debugger\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mining massive datasets week02\n",
    "===============\n",
    "\n",
    "\n",
    "##Locality-Sensitive Hashing\n",
    "\n",
    "Many data mining problems can be described as fining similar sets:\n",
    "\n",
    "* pages with similar words\n",
    "\t* similar document on the web\n",
    "\t* aggregating news story\n",
    "* similar taste with moves\n",
    "* moves with similar set of fans\n",
    "* entity resolution\n",
    "\t* are those records relating to the same person?\n",
    "\n",
    "We can get both false positive and false negative. To complete the task we will use 3 steps below\n",
    "\n",
    "\n",
    "###shingling\n",
    "Concerting document to sets\n",
    "\n",
    "* k-gram\n",
    "* we are often forced to use k>=10, due to size of document\n",
    "* we can compress shingle to token\n",
    "\n",
    "\n",
    "###minhashing\n",
    "\n",
    "* Convert large sets to short signatures, while preserving similarity\n",
    "* takes less space in memory\n",
    "* [Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index) is the size of set intersection divide by size of their union\n",
    "\t* $sim(C_1,C_2) = \\frac{ |C_1 \\cap C_2|}{ |C_1 \\cup C_2|}$\n",
    "* we can represent all as matrix where \n",
    "\t* rows are sets of all k-shingles (universal set)\n",
    "\t* columns are sets\n",
    "\t* it will be very sparse\n",
    "\t* sum all rows where both sets ==1 and div by all rows ==1\n",
    "* process\n",
    "\t* assume rows to be permuted randomly\n",
    "\t* minhas function h(c) = no of first row in which col c has 1\n",
    "\t\t* once we got all values we stop iterating\n",
    "\t* use several independent functions to create a signature for each column\n",
    "\t* signature matrix will represent this \n",
    "\t\t* columns are sets\n",
    "\t\t* rows are minhash values - in which iteration (which row after it has been permuted randomly) value of [1 appeared in particular row first](https://class.coursera.org/mmds-003/lecture/33)\n",
    "* the probability (over all permutations of rows) that $h(c_1) = h(c_2) == sim(c_1,c_2)$\n",
    "* we cant do so much permutations so instead we simulate it by using multiple has functions\n",
    "\tfor each row r:\n",
    "\t\tfor each hi:\n",
    "\t\t\tcompute hi(r)\n",
    "\t\tfor each column c:\n",
    "\t\t\tif c==1 in row r:\n",
    "\t\t\t\tfor each hi:\n",
    "\t\t\t\t\tif hi(r) < M(i,c):\n",
    "\t\t\t\t\t\tM(i,c) = hi(r)\n",
    "\n",
    "\n",
    "\n",
    "###locality-sensitivity hashing\n",
    "* focus on pair of signatures likely to be similar\n",
    "* we only work on subset, most close subset\n",
    "* how we do it?\n",
    "\t* we first define min required level of similarity\n",
    "\t* divide matrix M in b bands of r rows\n",
    "\t* for each row we hash this subset to hash table with k buckets\n",
    " \t* candidate column pars that hash to same bucket in >= 1 band\n",
    " \t* we tune r and b to catch most similar parts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Nearest-Neighbour Learning\n",
    "\n",
    "\n",
    "Distance describe similarities of the points. Distance have to:\n",
    "\n",
    "* d(x,y) >= 0\n",
    "* d(x,y) =0 iff x = y\n",
    "* d(x,y) = d(y,x)\n",
    "* d(x,y) <= d(x,z)+d(z,y)\n",
    "\n",
    "##Euclidean Distance\n",
    "\n",
    "* L2 norm - sqr of sum of squares of differences\n",
    "* L1 norm - asum of differences in each dimension\n",
    "\t* [Manhattan distance](http://www.chioka.in/differences-between-the-l1-norm-and-the-l2-norm-least-absolute-deviations-and-least-squares/)\n",
    "\n",
    "###Non-Euclidean Distance\n",
    "\n",
    "* [Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index)\n",
    "* [Cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "* [Edit distance](https://en.wikipedia.org/wiki/Edit_distance)\n",
    "* [Hamming Distance](https://en.wikipedia.org/wiki/Hamming_distance)\n",
    "\n",
    "##Interesting links\n",
    "\n",
    "* [How to Write a Spelling Corrector](http://norvig.com/spell-correct.html)\n",
    "* check this code <http://nearpy.io/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Quiz Week 2A: LSH (Basic)\n",
    "\n",
    "##Question 1\n",
    "The **edit distance** is the minimum number of character insertions and character deletions required to turn one string into another. Compute the edit distance between each pair of the strings *he, she, his, and hers*. Then, identify which of the following is a true statement about the number of pairs at a certain edit distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between he and she is: 1\n",
      "[('insert', 0, 0)]\n",
      "Distance between he and his is: 2\n",
      "[('insert', 1, 1), ('replace', 1, 2)]\n",
      "Distance between he and hers is: 2\n",
      "[('insert', 2, 2), ('insert', 2, 3)]\n",
      "Distance between she and his is: 3\n",
      "[('replace', 0, 0), ('replace', 1, 1), ('replace', 2, 2)]\n",
      "Distance between she and hers is: 3\n",
      "[('delete', 0, 0), ('insert', 3, 2), ('insert', 3, 3)]\n",
      "Distance between his and hers is: 2\n",
      "[('insert', 1, 1), ('replace', 1, 2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({2: 3, 3: 2, 1: 1})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations as Combinations\n",
    "#https://github.com/ztane/python-Levenshtein\n",
    "from Levenshtein import distance as EditDistance\n",
    "from Levenshtein import editops as EditOps\n",
    "from collections import Counter\n",
    "\n",
    "inputWords = ['he', 'she', 'his', 'hers']\n",
    "\n",
    "distanceGroups = []\n",
    "for wordA,wordB in list(Combinations(inputWords,2)): #processing all combinations of words\n",
    "    editDistance = EditDistance(wordA,wordB)\n",
    "    print \"Distance between %s and %s is: %i\" %(wordA,wordB,editDistance)\n",
    "    print EditOps(wordA,wordB)\n",
    "    distanceGroups.append(editDistance)\n",
    "\n",
    "#count them\n",
    "Counter(distanceGroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that question asked about insertion and deletion only, which is  different to standard defintion as it ignores replacement. We can calculate this by hand [using this presentation](https://web.stanford.edu/class/cs124/lec/med.pdf). This will change results for groups with replace giving us:\n",
    "\n",
    "* one pairs of distance 1\n",
    "* one pair of distance 2\n",
    "* three pairs of distance 3\n",
    "* one pair of distance 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Question 2\n",
    "Consider the given matrix and perform a minhashing of the data, with the order of rows: R4, R6, R1, R3, R5, R2. Which of the following is the correct minhash value of the stated column? Note: we give the minhash value in terms of the original name of the row, rather than the order of the row in the permutation. These two schemes are equivalent, since we only care whether hash values for two columns are equal, not what their actual values are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [0 0 1 0]\n",
      " [1 0 1 0]\n",
      " [0 1 0 0]]\n",
      "[3, 5, 0, 2, 4, 1]\n",
      "[[0 0 1 0]\n",
      " [0 1 0 0]\n",
      " [0 1 1 0]\n",
      " [0 1 0 1]\n",
      " [1 0 1 0]\n",
      " [1 0 1 1]]\n",
      "Note that answer is based on ORGINAL rows!\n",
      "Column c1 == R5\n",
      "Column c2 == R6\n",
      "Column c3 == R4\n",
      "Column c4 == R3\n"
     ]
    }
   ],
   "source": [
    "M =  np.matrix('0,1,1,0;\\\n",
    "               1,0,1,1;\\\n",
    "               0,1,0,1;\\\n",
    "               0,0,1,0;\\\n",
    "               1,0,1,0;\\\n",
    "               0,1,0,0')\n",
    "print M\n",
    "mininhashOrder = [x -1 for x in [4,6,1,3,5,2]] #python idx from 0\n",
    "print mininhashOrder\n",
    "\n",
    "orderedM = M[mininhashOrder,:]\n",
    "print orderedM\n",
    "print \"Note that answer is based on ORGINAL rows!\"\n",
    "minihashValues = [4,1,0,3]\n",
    "\n",
    "for idx in range(len(minihashValues)):\n",
    "    print \"Column c%i == R%i\" %(idx+1,mininhashOrder[minihashValues[idx]]+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mininhashOrder[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Question 4\n",
    "Find the set of 2-shingles for the \"documents\". Answer the following questions:\n",
    "\n",
    "* How many 2-shingles does ABRACADABRA have?\n",
    "* How many 2-shingles does BRICABRAC have?\n",
    "* How many 2-shingles do they have in common?\n",
    "* What is the Jaccard similarity between the two documents\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/\n",
    "def CaclulateNgrams(document,ngram_size):\n",
    "  ngram = zip(*[document[idx:] for idx in range(ngram_size)])\n",
    "  return list(set(ngram)) #remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 'A'), ('A', 'D'), ('B', 'R'), ('D', 'A'), ('R', 'A'), ('A', 'B'), ('A', 'C')]\n",
      "[('R', 'I'), ('I', 'C'), ('C', 'A'), ('B', 'R'), ('R', 'A'), ('A', 'B'), ('A', 'C')]\n",
      "We have 7 ngrams for document A and 7 for document B\n",
      "5 are in common between documents, leading to Jaccard similarity of 0.36\n",
      "[('R', 'A'), ('C', 'A'), ('A', 'B'), ('B', 'R'), ('A', 'C')]\n"
     ]
    }
   ],
   "source": [
    "documentA = 'ABRACADABRA'\n",
    "documentB = 'BRICABRAC'\n",
    "ngram_documentA = CaclulateNgrams(documentA,2)\n",
    "ngram_documentB = CaclulateNgrams(documentB,2)\n",
    "ngram_same =  list(set(ngram_documentA)&set(ngram_documentB))\n",
    "print ngram_documentA\n",
    "print CaclulateNgrams(documentB,2)\n",
    "print \"We have %i ngrams for document A and %i for document B\" %(len(ngram_documentA),len(ngram_documentB))\n",
    "\n",
    "similiarityJaccard = len(ngram_same)/float(len(ngram_documentA)+len(ngram_documentB))\n",
    "print \"%i are in common between documents, leading to Jaccard similarity of %.2f\"  %(len(ngram_same),similiarityJaccard)\n",
    "print ngram_same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Question 6\n",
    "Suppose we want to assign points to whichever of the points (0,0) or (100,40) is nearer. Depending on whether we use the L1 or L2 norm, a point (x,y) could be clustered with a different one of these two points. For this problem, you should work out the conditions under which a point will be assigned to (0,0) when the L1 norm is used, but assigned to (100,40) when the L2 norm is used. Identify one of those points from the list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist as Distance\n",
    "\n",
    "def IsThePointCorrect(point):\n",
    "    pointA = [(0,0)]\n",
    "    pointB = [(100,40)]\n",
    "    \n",
    "    #print Distance(pointA,point,'cityblock')[0]\n",
    "    #print Distance(pointB,point,'cityblock')[0]\n",
    "    #print Distance(pointA,point,'euclidean')[0]\n",
    "    #print Distance(pointB,point,'euclidean')[0]\n",
    "    if (Distance(pointA,point,'cityblock')[0]<Distance(pointB,point,'cityblock')[0]) \\\n",
    "    & (Distance(pointB,point,'euclidean')[0]<Distance(pointA,point,'cityblock')[0]):\n",
    "        isThePointCorrect = True\n",
    "    else: isThePointCorrect = False\n",
    "    return isThePointCorrect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print IsThePointCorrect([(56,13)])\n",
    "print IsThePointCorrect([(61,10)])\n",
    "print IsThePointCorrect([(58,13)])\n",
    "print IsThePointCorrect([(58,13)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Quiz Week 2B: LSH (Basic)\n",
    "\n",
    "##Question 1\n",
    "Suppose we have transactions that satisfy the following assumptions: \n",
    "* s, the support threshold, is 10,000.\n",
    "* There are one million items, which are represented by the integers 0,1,...,999999.\n",
    "* There are N frequent items, that is, items that occur 10,000 times or more.\n",
    "* There are one million pairs that occur 10,000 times or more.\n",
    "* There are 2M pairs that occur exactly once. M of these pairs consist of two frequent items, the other M each have at least one nonfrequent item.\n",
    "* No other pairs occur at all.\n",
    "\n",
    "Integers are always represented by 4 bytes.\n",
    "Suppose we run the a-priori algorithm to find frequent pairs and can choose on the second pass between the triangular-matrix method for counting candidate pairs (a triangular array count[i][j] that holds an integer count for each pair of items (i, j) where i < j) and a hash table of item-item-count triples. Neglect in the first case the space needed to translate between original item numbers and numbers for the frequent items, and in the second case neglect the space needed for the hash table. Assume that item numbers and counts are always 4-byte integers. \n",
    "As a function of N and M, what is the minimum number of bytes of main memory needed to execute the a-priori algorithm on this data? Demonstrate that you have the correct formula by selecting, from the choices below, the triple consisting of values for N, M, and the (approximate, i.e., to within 10%) minumum number of bytes of main memory, S, needed for the a-priori algorithm to execute with this data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
